---
layout: post
title:  "机器学习笔记（二） 逻辑回归"
date:   2018-02-18 17:32:00 -0600
categories: CS Math
published: false
---
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

> 这系列是一组我在Coursera上学习机器学习时总结的一些笔记。由于涉及到Coursera的Honor Code, 我尽量使用我自己的语言来总结。论证过程中如有疏忽与缺漏，请不吝指正。

再看另一个例子。同样给定一组数据\\({(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)}\\)。但是这次\\(y_n\\)的取值只为\\(\{0, 1\}\\)。如果把\\(y_n\\)看做随机事件\\(Y\\)的抽样，那么\\(Y\\)服从伯努利分布。

同样的在已知\\(x\\)的情况下，我们构建一个模型来预测一下\\(y\\)。与线性回归不同，我们这次把线性模型输出的结果输入到一个激励函数（Actication Function）里面去。这里我们采用Sigmoid函数。

$$
g(z) = \frac{1}{1+e^{-z}}
$$

最后我们设置一个决策边界（Decision Boundary），如果g(z)的输出大于0.5那么就认为模型预测了1，否则为0。

如何判断这个模型的好坏？能不能还是继续用均方误差作为损失函数？不行，g(z)的输出几乎不会精确的等于0或者1，因此误差始终无法消除。如果利用决策边界二值化后的数值，（1）这个过程不连续，不能求导，对各种机器学习算法都不利，（2）无法体现我们希望模型对正例子尽可能输出1，反例子尽可能输出0的思想。

我们说过\\(Y\\)服从伯努利分布，\\(P(y=1)=\theta\\)，m个独立同分布的Y事件发生的概率密度函数为：

$$
P(\mathbf Y | \theta) = \prod^m_{i=1}\theta^{y_i}(1-\theta)^{1-y_i} \\

\mathbf Y = (y_1, y_2, ... y_m)^T
$$

现在的情况是，我们手头有一大堆\\(Y\\)的采样\\(y_1, y_2, ..., y_m\\)，只是不知道\\(Y\\)的分布。那么如果把上面这个概率密度函数看做是一个关于\\(\theta\\)的函数，那么就得到了一个似然函数。

$$
L(\theta|\mathbf Y) = \prod^m_{i=1}\theta^{y_i}(1-\theta)^{1-y_i}
$$

这式子跟上面的区别就是先验后验交换了位置。这条式子描述的内容也变成了“Y服从某种分布的时候（\\(\theta\\)取某个值时），满足所有观测值的可能性”。当\\(\theta\\)取某个值的时候能使这个可能性最大。这是MLE的内容。

我们其实并不care \\(\theta\\)的值是什么，我们care的是逻辑回归里的参数。但是这条式子值得利用，原因这条式子是连续的，可以求\\(\theta\\)的偏导。假如说我们让逻辑回归模型输出的就是\\(\theta\\)的估计值\\(\hat{\theta}\\)，那么多次梯度下降迭代之后\\(\hat{\theta}\\)会逼近MLE的结果。

$$
L(\theta|\mathbf Y, \mathbf X) = \prod^m_{i=1}g(z_i)^{y_i}(1-g(z_i))^{1-y_i}
$$

考虑到log函数在增减性上的保持，出于对式子简化的想法，我们转而求上面函数的负对数形式：

$$
J(\theta) = -\sum^m_{i=1}y_ilog(g(z_i))+(1-y_i)log(1-g(z_i))
$$

使用负数的原因是跟“损失函数”的定义相符，目标是最小化。

### 激励函数

常用的激励函数有Sigmoid, tanh, relu, leaky relu, elu等等。激励函数在神经网络中也是不可缺少的组成部分。

为什么要激励函数？从数学上来看是起了一个截断（clipping）的作用。Sigmoid函数将输出被限制到了\\([0, 1]\\)之间。假如说一个模型的值域是无穷大的话，那么我们并不好设置一个阈值去将模型的输出归到0和1这个逻辑值。

或者从生物角度去理解：神经信号在跨越突触间隙的时候都要满足一定的阈值，否则就会在细胞内消失。这可以用来解释Relu函数截断负响应的效果。

当然，不管是Sigmoid还是Relu，得以大规模应用还是在严谨的实验基础上被证明有效才会被采用的[1]。其中Relu应用更为广泛，因为它有效地避免了Sigmoid函数带来的梯度消失（Vanishing Gradient）的问题。

[1] Nair et. el. Rectified Linear Units Improve Restricted Boltzmann Machines
